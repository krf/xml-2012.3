\documentclass[a4paper,11pt]{scrartcl}
\usepackage[left=2.5cm, right=3cm]{geometry}              % flexible and complete interface to document dimensions
\usepackage[latin1]{inputenc}                             % font encoding for umlauts
\usepackage[ngerman]{babel}                               % language settings
\usepackage[T1]{fontenc}                                  % T1 font encoding
\usepackage{amsmath}                                      % AMS mathematical facilities
%\usepackage{amssymb}                                     % defines symbol names for math symbols in the fonts MSAM and MSBM
\usepackage[bitstream-charter]{mathdesign}                % mathematical fonts to fit with Bitstream Charter
\usepackage{courier}                                      % replaces the current typewriter font for Adobe Courier
\usepackage{listings}                                     % source code printer
\usepackage{color}                                        % adding colors
\usepackage{fancyhdr}                                     % extensive control of page headers and footers
\usepackage{booktabs}                                     % publication quality tables
\usepackage{xcolor}                                       % driver-independent color extensions   
\usepackage{tikz}                                         % creating graphics programmatically
\usepackage{float}                                        % improved interface for floating objects
\usepackage{subfig}                                       % figures broken into subfigures
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}      % floating algorithm environment with algorithmic keywords
\usepackage{hyperref}
\hypersetup{
  colorlinks=false,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black,
}
% url formatting
\urlstyle{rm}

\definecolor{lightgray}{rgb}{.95,.95,.95}

\lstset{language=python,
        backgroundcolor=\color{lightgray},
        basicstyle=\ttfamily\fontsize{9pt}{9pt}\selectfont\upshape,
				commentstyle=\rmfamily\slshape,
				keywordstyle=\rmfamily\bfseries\color{black},
				captionpos=b,
				showstringspaces=false,
				breaklines=true,
				frame=lines,
				tabsize=2,
				aboveskip={\baselineskip}
}

\renewcommand{\labelenumi}{(\arabic{enumi})}
\renewcommand{\labelenumii}{(\alph{enumii})}
\renewcommand{\algorithmcfname}{Algorithmus}

\subject{Dokumentation}
\title{XML Technologien Projekt}
\author{David Bialik $\cdot$ Kevin Funk\\ Jan Kostulski $\cdot$ Konrad Reiche $\cdot$ André Zoufahl}
\date{\today}
\publishers{\vskip 2ex -- Gruppe 3 --}

\begin{document}

\pagestyle{fancy}
\maketitle

\section{Einleitung}

Dieses Dokument beschreibt die Lösung des Softwareprojekts für den Kurs XML Technologien Sommersemester 2012 bearbeitet von Gruppe 3. Aufgabenstellung war es über den Dienst \emph{gpsies.com} Informationen über verschiedene Routen in Form von XML Dokumenten zu beschaffen, diese unter Nutzung von semantischer Anreicherung zu erweitern und schlussendlich auf einer selbst entwickelten Weboberfläche anzubieten.

Im weiteren werden wir auf Implementierungsdetails eingehen, erläutern welche Probleme aufgetreten sind und deren Lösungen diskutieren. Am Ende gibt es eine Anleitung zur Verwendung der Anwendung. Zunächst wird die Systemarchitektur unserer Lösung skizziert.

\section{Systemarchitektur}

Die Systemarchitektur ist in Abbildung \ref{fig:architecture} skizziert. An ihr kann auch der Ablauf der Anwendung erläutert werden:

\begin{enumerate}
\item der Crawler, als unabhängige Komponente, wird in zwei Schritten ausgeführt

\begin{enumerate}
\item laden der \emph{Result Pages} von gpsies.org und Persistieren auf der XML Datenbank

\item laden der \emph{Track Details} pro \emph{File Id} von gpsies.org und Persistieren auf der XML Datenbank
\end{enumerate}

\item das SPARQL Skript, ebenfalls als unabhängige Komponente, konstruiert pro Track 2 SPARQL Queries (Anfangs- und Endkoordinaten) und sendet diese Anfrage an DBpedia, die Antwort wird als JSON bearbeitet und als Erweiterung in die XML Datenbank zurückgeschrieben

\item der Web Server sendet auf Anfrage durch den Benutzer das HTML Formular für eine detaillierte Suche in den Tracks, dabei wird die HTML Seite mithilfe von XSLT konstruiert.

\item auf Suchanfrage durch den Benutzer werden entsprechende Dokumente durch XQuery Anfrage an die Datenbank geladen, Point of Interest Daten werden verwendet um Zusatzinformationen vom Kurznachrichtendienst Twitter zu erhalten, beim Konstruieren des HTML Ergebnis wird Microdata zur semantischen Erweiterung hinzugefügt.

\end{enumerate}

\begin{figure}[H]
\includegraphics[width=\columnwidth]{resources/architecture.pdf}
\caption{Skizze der Architektur mit den verschiedenen Komponenten und Interaktionen}
\label{fig:architecture}
\end{figure}

Jeder Track wird als einzelnes Dokument in der XML Datenbank abgelegt. Es gibt zwei Validierungsschritte, zum einen die Validierung der Trackdokumente nach einem Schema wie sie von \emph{gpsies.org} direkt ausgeliefert werden und zum anderen das XML Schema nach der augmentierung durch das SPARQL Skript. Für das SPARQL Skript wurden mehrere Threads verwendet um mehrere Tracks gleichzeitig zu erweitern.

\section{Implementierung}

Für die Implementierung haben wir uns für die Programmiersprache Python entschieden, d.h. alle Komponenten die programmatisch ausgeführt werden müssen, wurden in Python realisiert. Von einer Auswahl von verschiedenen XML Datenbanken wurde BaseX gewählt. Als Webserver Framework wurde \emph{Tornado} gewählt. Im weiteren werden Details zur Implementierung der einzelnen Komponenten erläutert.

\subsection{Crawler}

Das Script \emph{crawler.py} umfasst folgende Funktionen

\begin{lstlisting}[language=]
usage: crawler [-h] [-c] [-e] [-s] [-p]

XML Crawler for gpsies.com

optional arguments:
  -h, --help        show this help message and exit
  -c, --crawl       Crawl from gpsies.com, parse result pages
  -e, --extend      Crawl from gpsies.com, extend non-augmented tracks with
                    track details
  -s, --statistics  Print database statistics
  -p, --prune       Clear database, remove all non-augmented tracks
\end{lstlisting}

Der Crawler implementiert zwei Hauptaktionen (\emph{crawl, extend}), um die Datenbank mit XML-Daten von \emph{gpsies.org} zu füllen.

\subsubsection{Crawl}
\begin{itemize}
  \item Es werden Resultpages mit jeweils 100 Tracks von \emph{gpsies.org} abgefragt
\footnote{Beispiel-URL: http://www.gpsies.org/api.do?key=API\_KEY\&country\=DE\&limit=100\&resultPage=1}
  \item Davon werden die einzelnen \emph{<track/>}-Elemente geparst
  \item Diese werden einzeln als Dokument in die Datenbank geschrieben
  \item Der Dokumentenname ist hierbei die Track-FileID
\end{itemize}

\subsubsection{Extend (Augment)}
\begin{itemize}
  \item Hier werden sukzessiv nicht-augmentierte Tracks aus der Datenbank angereichert
  \item Pro Track werden die Trackdetails von \emph{gpsies.org} geholt
\footnote{Beispiel-URL: http://www.gpsies.org/api.do?key=API\_KEY\&fileId=cimmxugjixiyzakj\&trackDataLength=250}
  \item Danach wird der Track durch die detaillreichere Version in der Datenbank ersetzt
\end{itemize}

\subsection{Schnittstelle zur Datenbank}

BaseX bietet verschiedene Befehle an, um auf die Datenbank zuzugreifen, Dokumente hinzuzufügen, zu modifizieren oder zu löschen. BaseX bietet selber einen relative dünnen Python Wrapper an, um die Befehle programmatisch von Python zu nutzen. Für unsere Anwendung hätten aber viele Befehle in Komposition genutzt werden müssen, daher lag es nahe, alle für uns relevanten Funktionalitäten in einer Python Schnittstelle für die Datenbank zu abstrahieren:

\begin{lstlisting}[caption=Das Modul interface.py]
class TrackInterface:
  def addTrack(self, track):
  def removeTrack(self, track):
  def removeTrackById(self, fileId):
  def addTracks(self, tracks):
  def findTrack(self, fileId):
  def getTracks(self):
  def getTrackCount(self):
  def getNonAugmentedTracks(self):
  def getNonAugmentedTrackCount(self):
  def getAugmentedTracks(self):
  def getAugmentedTrackCount(self):
  def getNonPoiTracks(self):
  def getNonPoiTrackCount(self):
  def getPoiTracks(self):
  def getPoiTrackCount(self):
\end{lstlisting}

Implementierungen wie diese Schnittstelle entstanden in mehreren Iterationen, d.h. zunächst wurde der Python Wrapper genutzt und später nach Hinzufügen der Schnittstelle refaktorisiert.

\subsection{SPARQL Anfrage}

Dank eines SPARQL Wrappers für Python können die SPARQL Anfragen in Python konstruiert, gesendet und deren Ergebnis verarbeiten werden. Für jeden Track der noch keine Point-of-Interests Elemente hat, d.h. im XML Dokument noch keinen \emph{<poi/>} Knoten vorzuweisen hat, werden die Start- und Endkoordinaten analysiert. Eine Beispiel SPARQL Anfrage sieht dann bspw. wie folgt aus:

\begin{lstlisting}[caption=Beispiel SPARQL Anfrage]
PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>
SELECT ?subject ?label ?lat ?long ?abstract ?wiki ?image ?typeLabel WHERE {
?subject geo:lat ?lat.
?subject geo:long ?long.
?subject rdfs:label ?label.
?subject dbpedia-owl:abstract ?abstract.
?subject foaf:page ?wiki.
?subject rdf:type ?type.
?subject foaf:depiction ?image.
?type rdfs:label ?typeLabel.
FILTER(?lat - 50.333777 <= 0.050000 && 50.333777 - ?lat <= 0.050000 &&
      ?long - 6.945088 <= 0.050000 && 6.945088 - ?long <= 0.050000 &&
      lang(?abstract) = "de" &&
      lang(?label) = "de" &&
      lang(?typeLabel) = "de"
      ).
} LIMIT 10
\end{lstlisting}

Durch den Filter wird eine Umkreissuche realisiert, dabei werden alle in der DBpedia befindlichen Ressourcen gelistet, deren GPS Koordinaten, sofern vorhanden, im Umkreis von $0,05$ Latitude/Longitude liegen. Das entspricht in Deutschland circa einem Umkreis von 7,5km.

Von den gefundenen Ressourcen werden folgende Daten verwendet, die dann auch als Point-of-Interest dem Trackdokument angereichet werden:

\begin{itemize}
\item \textbf{Label} Bezeichner
\item \textbf{Latitude} Latitude Koordinate in Dezimalschreibweise
\item \textbf{Longitude} Longitude Koordinate in Dezimalschreibweise
\item \textbf{Abstract} Eine zusammenfassende Beschreibung der Ressource, in Wikipediaartikeln meist der erste Paragraph
\item \textbf{Wiki} Die URL zum entsprechenden Wikipediaartikel
\item \textbf{Image} Die URL zu einem Bild der Ressource, sofern vorhanden
\end{itemize}

Als sehr problematisch hat sich die anfängliche die Laufzeit herausgestellt. Die Anfragen werden von DBpedia nach Beobachtungen mit deutlicher Verzögerung beantwortet. Mit einfachen Hochrechnungen hätte so das Anreichern von $100.000$ Tracks circa. 4 Monate benötigt.

Dies konnte durch den Einsatz von mehreren Threads gelöst werden. Das SPARQL Skript wird multithreaded ausgeführt mit $n$ Threads für das Senden der Anfragen und einem Thread für das Zurückschreiben der Tracks in die Datenbank.

DBpedia erkennt jedoch, wenn mehere Anfragen gleichzeitig gestellt werden und blockt diese mit der Fehlermeldung HTTP 503 \emph{Service Unavailable} ab. Erhält ein Thread diese Fehlermeldung ruft er zwischen $5$ und $20$ Sekunden \emph{sleep} auf und versucht es dann erneut. Auf diese weise pendeln die Threads sich ein, bis es keine Fehlermeldung mehr gibt. Damit konnte die benötigte Zeit von $4$ Monaten auf $40$h reduziert werden.

\subsection{Webserver}

Der Webserver ist unter Verwendung von \emph{Tornado} sehr schnell realisiert. Die verschiedene Pfade, wie z.B. \emph{/request} oder \emph{/stats} werden auf verschiedene Handler abstrahiert, die sich mit den Anfragen separat auseinandersetzen.

\begin{lstlisting}[caption=Starten des Webservers und Zuordnung der Kontexte]
application = tornado.web.Application([
    (r"/", MainHandler),
    (r"/request", RequestHandler),
    (r"/detail", DetailHandler),
    (r"/kml", KmlHandler),
    (r"/stats", StatisticsHandler)
], **settings)

if __name__ == "__main__":
    port = 8888
    application.listen(port)

    try:
        print("Starting web server on localhost:{0}".format(port))
        tornado.ioloop.IOLoop.instance().start()
    except KeyboardInterrupt:
        print("\nKeyboardInterrupt. Exit.")
    sys.exit(0)
\end{lstlisting}

\subsection{Webanwendung}

Die benötigte HTML Seiten werden ausschließlich dynamisch durch serverseitige XSL Transformationen konstruiert und an den Anwender übertragen. Dies wurde jedoch nicht nur für HTML Seiten verwendet, sondern auch für das weitere Aufbereiten der Daten.

\begin{lstlisting}[label=lst:kml, caption=Transformation von XML nach KML, language=XSLT]
<xsl:template match="response">
    <kml xmlns="http://www.opengis.net/kml/2.2">
    <Document>
        <name>POI</name>
        <description>Points of interest zur Route dieser ID</description>
    <xsl:for-each select="pois/poi">
        <Placemark>
            <name><xsl:value-of select="name"/></name>
            <description>
                <xsl:value-of select="abstract"/>
<div class="controls">
    <a href="#poi{position()}" class="anchorlink">Was wird über diesen Ort gesagt?</a></div>              
            </description>
            <Point>
                <coordinates><xsl:value-of select="lon"/>,<xsl:value-of select="lat"/>,0</coordinates>
            </Point>
        </Placemark>
    </xsl:for-each>
    </Document>
    </kml>
</xsl:template>
\end{lstlisting}

So wie zum Beispiel in Listing \ref{lst:kml}, die Trackdaten werden nach Keyhole Markup Language (KML) überführt, ein XML Format um geographische Daten bspw. auf Google Maps darzustellen.

\section{Test-Umgebung}

Unsere Anwendung wurde in den folgenden Systemen getestet und sollte unter den entsprechenden Voraussetzungen in jedem Fall funktionsfähig sein:

\subsection{Referenzsystem}

Unsere Anwendung wurde hauptsächlich auf folgenden Plattformen getestet:

\begin{itemize}
  \item Ubuntu 12.04

  \begin{itemize}
    \item Python 2.7.3
    \item BaseX 7.0.2
  \end{itemize}

  \item Windows 7
  \begin{itemize}
    \item Python 2.7.3
    \item BaseX 7.0.2
  \end{itemize}
\end{itemize}

\section{Installationsanleitung}

Im folgenden wird die Installation der Anwendung erläutert. Dazu gehört auch das Einrichten der Datenbank mit angereicherten Tracks, sofern nicht der bestehende Dump der fertigen Datenbank genutzt wird.

\subsection{Abhängigkeiten}

\begin{itemize}
  \item Base (\url{http://basex.org/})
  \item Tornado Web Server (\url{http://www.tornadoweb.org/})
  \item Python LXML (\url{http://lxml.de})
  \item SPARQL Endpoint interface to Python (\url{http://sparql-wrapper.sourceforge.net/})
\end{itemize}

\subsubsection{Quick install (für Debian-basierte Systeme)}

Getestet auf Ubuntu 12.04.

\begin{lstlisting}
* apt-get install basex python-tornado python-lxml python-sparqlwrapper
\end{lstlisting}


\subsection{Datenbank aufsetzen}

Ein fertiger Datensatz kann hier heruntergeladen werden:

\begin{center}
\url{http://kuonrat.userpage.fu-berlin.de/default-2012-06-28-15-26-57.zip}
\footnote{Der Datenbankdump beinhaltet 55.823 Tracks wovon 1.696 keine POIs haben, d.h. für diese Tracks wurden keine interessanten Punkte im Umkreis gefunden.}
[373 MB]
\end{center}

Zunächst muss der Datenbankdump (\emph{default-*.zip}) nach \emph{\$HOME/BaseXData/} kopiert werden um sie später importieren zu können.

Starten der BaseX-Datenbank

\begin{lstlisting}
$ basexserver
\end{lstlisting}

Erstmaliges Erstellen der Datenbank

\begin{lstlisting}
$ basexclient

# Notiz: Der Standardlogin ist admin:admin

$ > create database default
\end{lstlisting}

Importieren unseres Datenbankdumps:
\begin{lstlisting}
$ > restore default
\end{lstlisting}

\subsection{Ausführen des Crawlers (optional)}

Zum Ausführen des Crawlers muss eine Internetverbindung bestehen und die Verfügbarkeit von \emph{gpsies.org} gewährleistet sein.
Außerdem muss eine BaseX-Server-Instanz für die Datenbankanbindung laufen.

Danach kann der Crawler folgendermaßen gestartet werden:
\footnote{Notiz: Alle ausführbaren Dateien liegen im Projektorder \emph{src/}}

\begin{lstlisting}
# Parse result pages from gpsies.org
$ ./crawler.py -c
\end{lstlisting}

Der Crawler läuft mit diesem Parameter solange, bis er mindestens $100000$ Tracks in der Datenbank gespeichert hat. Diesen Tracks fehlen dann aber noch Detaillinformationen, wie die Startpunktaddresse, welche aber für die Suchmaske auf dem Webformular gebraucht werden.

Um diese Informationen zu bekommen muss danach der Crawler mit folgenden Parameters gestartet werden.

\begin{lstlisting}
# Augment tracks with information from gpsies.org
$ ./crawler.py -e
\end{lstlisting}

Bei diesem Aufruf fragt der Crawler für jeden Track in der Datenbank die Detaillinformationen auf \emph{gpsies.org} ab und reichert damit die jeweiligen Tracks an.

\subsection{Anreicherung mit Point-Of-Interests (optional)}

Das SPARQL Skript kann direkt gestartet werden:

\begin{lstlisting}
# Enrich tracks with point of interests
$ ./sparql.py
\end{lstlisting}

Daraufhin werden die gefundenen Tracks angezeigt, sowie eine sich aktualisierende Statusanzeige mit Zeitabschätzung bis zur Terminierung, einer Hochrechnung für 100.000 Tracks, sowie ausstehnden Tracks die noch zurückgeschrieben werden müssen.

\begin{lstlisting}
Found 5300 tracks without POIs
Tracks processed: 4665 / 5300 (~ 2h 22min) => 100.000 Tracks (~ 46h 38min) (DB Writer Queue: 1)
\end{lstlisting}

Das Skript kann jederzeit unterbrochen werden und setzt bei erneutem Starten bei den Tracks fort, die noch nicht angereichert wurden.

\subsection{Webserver starten}

Nachdem die Datenbank befüllt ist, kann der Webserver gestartet werden.

\begin{lstlisting}
# Start web server
$ ./web.py
\end{lstlisting}

Dies startet einen Tornado-Webserver auf der Adresse \url{http://localhost:8888} welche mit dem Browser angesteuert werden kann.

\section{Benutzerdokumentation}

Wird die Anwendung gestartet, kann der Benutzer über drei verschiedene Felder eingaben machen: Freitext, Ort und Suche. Die Ergebnisse werden
unter Angabe der Titel angezeigt, inklusive der augmentierten Postleitzahlen und der Anzahl verfügbarer POIs.

\begin{figure}[H]
\center
\includegraphics[width=0.7\textwidth]{resources/screenshot-1.png}
\caption{Darstellung der Suchergebnisse}
\label{fig:screenshot-1}
\end{figure}

Über den Titellink können Details über die POIs abgerufen werden. Die POIs werden mithilfe der Google Maps API visualisiert.
Weitere Informationen wie 

\begin{figure}[H]
\center
\includegraphics[width=0.8\textwidth]{resources/screenshot-3.png}
\caption{Visualisierung unter Verwendung der Google Maps API}
\label{fig:screenshot-3}
\end{figure}

Informationen zu den POIs befinden sich im unteren Bereich, z.B. inhaltliche Zusammenfassung zu dem POI, Bilder, Wikipedialink, usw.

\begin{figure}[H]
\center
\includegraphics[width=0.7\textwidth]{resources/screenshot-2.png}
\caption{Anzeige der Details von Point of Interests}
\label{fig:screenshot-2}
\end{figure}

\section{Anhang}

Der Quellcode für dieses Projekt ist einsehbar unter folgender Adresse:

\begin{center}
\url{https://github.com/krf/xml-2012.3}
\end{center}

\end{document}
